{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2501933",
   "metadata": {},
   "source": [
    "# 1 Feedforward Neural Networks (FNNs)\n",
    "\n",
    "1.1 Architecture: Input Layer → Hidden Layers (with activation functions such as ReLU, Sigmoid, Tanh) → Output Layer (with softmax or linear activation)\n",
    "\n",
    "1.2 Description: Fully connected layers with no cycles or feedback loops, purely feedforward processing of data.\n",
    "\n",
    "1.3 Applications: Classification, Regression, Function approximation.\n",
    "\n",
    "1.4 Examples: Predicting house prices, Email spam detection.\n",
    "\n",
    "## 1.5 Techniques:\n",
    "\n",
    "1.5.1 Weight Initialization (He, Xavier)\n",
    "\n",
    "1.5.2 Activation Functions (ReLU, Sigmoid, Tanh)\n",
    "\n",
    "1.5.3 Loss Functions (Cross-Entropy Loss for classification, Mean Squared Error for regression)\n",
    "\n",
    "1.5.4 Optimization Algorithms (Stochastic Gradient Descent, Adam, RMSprop)\n",
    "\n",
    "1.5.5 Regularization (L2 Regularization, Dropout)\n",
    "\n",
    "# 2 Convolutional Neural Networks (CNNs)\n",
    "\n",
    "2.1 Architecture: Convolutional Layers (with filters/kernels) → Pooling Layers (Max Pooling, Average Pooling) → Fully Connected Layers (Dense Layers)\n",
    "\n",
    "2.2 Description: Utilizes convolutional filters to capture spatial hierarchies and patterns.\n",
    "\n",
    "2.3 Applications: Image Classification, Object Detection, Image Segmentation.\n",
    "\n",
    "2.4 Examples: Recognizing digits in MNIST dataset, Detecting objects in COCO dataset.\n",
    "\n",
    "## 2.5 Layers\n",
    "\n",
    "2.5.1 Convolutional Layer (with parameters like filter size, stride, padding)\n",
    "\n",
    "2.5.2 Pooling Layer (Max Pooling, Average Pooling, Global Pooling)\n",
    "\n",
    "2.5.3 Fully Connected Layer (Dense Layer)\n",
    "\n",
    "## 2.6 Techniques\n",
    "\n",
    "2.6.1 Data Augmentation (Rotation, Scaling, Flipping, Cropping)\n",
    "\n",
    "2.6.2 Transfer Learning (using pre-trained models like VGG, ResNet, Inception)\n",
    "\n",
    "2.6.3 Fine-Tuning (adjusting a pre-trained model to a new task)\n",
    "\n",
    "2.6.4 Batch Normalization\n",
    "\n",
    "2.6.5 Dropout\n",
    "\n",
    "2.6.6 Learning Rate Schedulers (Step Decay, Exponential Decay, Cyclical Learning Rates)\n",
    "\n",
    "# 3 Recurrent Neural Networks (RNNs)\n",
    "\n",
    "3.1 Architecture: Input → Recurrent Hidden Layers (with activation functions like Tanh or GRU/LSTM cells) → Output\n",
    "\n",
    "3.2 Description: Processes sequential data by maintaining a hidden state that captures information from previous time steps.\n",
    "\n",
    "3.3 Applications: Time Series Forecasting, Language Modeling, Sequence Classification.\n",
    "\n",
    "3.4 Examples: Predicting stock prices, Text generation.\n",
    "\n",
    "## 3.5 Variants\n",
    "\n",
    "### 3.5.1 Long Short-Term Memory (LSTM)\n",
    "---------------------------------------\n",
    "3.5.1.1 Description: Mitigates vanishing gradient problem using gating mechanisms.\n",
    "\n",
    "3.5.1.2 Gates: Input Gate, Forget Gate, Output Gate.\n",
    "\n",
    "3.5.1.3 Applications: Speech Recognition, Machine Translation.\n",
    "\n",
    "3.5.1.4 Examples: Google Translate.\n",
    "\n",
    "### 3.5.2 Gated Recurrent Units (GRU)\n",
    "---------------------------------------\n",
    "3.5.2.1 Description: Simplified version of LSTM with fewer gates.\n",
    "\n",
    "3.5.2.2 Gates: Update Gate, Reset Gate.\n",
    "\n",
    "3.5.2.3 Applications: Similar to LSTM.\n",
    "\n",
    "3.5.2.4 Examples: Time series prediction, Sentiment analysis.\n",
    "\n",
    "## 3.6 Techniques\n",
    "\n",
    "3.6.1 Backpropagation Through Time (BPTT)\n",
    "\n",
    "3.6.2 Gradient Clipping (to handle exploding gradients)\n",
    "\n",
    "3.6.3 Regularization (Dropout, L2 Regularization)\n",
    "\n",
    "3.6.4 Sequence Padding and Masking\n",
    "\n",
    "3.6.5 Attention Mechanisms (for focusing on specific parts of the sequence)\n",
    "\n",
    "# 4 Autoencoders\n",
    "\n",
    "4.1 Architecture: Encoder (with convolutional or fully connected layers) → Bottleneck (Latent Space) → Decoder (mirroring the encoder structure)\n",
    "\n",
    "4.2 Description: Learns to compress input data into a lower-dimensional representation and then reconstruct it from this representation.\n",
    "\n",
    "4.3 Applications: Dimensionality Reduction, Anomaly Detection, Data Denoising.\n",
    "\n",
    "4.4 Examples: Reducing dimensionality of image datasets, Detecting fraud.\n",
    "\n",
    "## 4.5 Variants\n",
    "\n",
    "### 4.5.1 Variational Autoencoders (VAEs)\n",
    "-----------------------------------------\n",
    "4.5.1.1 Description: Probabilistic approach to autoencoders with latent variables following a specific distribution.\n",
    "\n",
    "4.5.1.2 Applications: Image Generation, Data Compression.\n",
    "\n",
    "4.5.1.3 Examples: Generating new faces.\n",
    "\n",
    "### 4.5.2 Denoising Autoencoders\n",
    "---------------------------------\n",
    "4.5.2.1 Description: Trained to remove noise from input data.\n",
    "\n",
    "4.5.2.2 Applications: Image Denoising.\n",
    "\n",
    "### 4.5.3 Sparse Autoencoders\n",
    "--------------------------------------------\n",
    "4.5.3.1 Description: Enforces sparsity constraint on hidden representations to learn useful features.\n",
    "\n",
    "4.5.3.2 Applications: Feature Learning.\n",
    "\n",
    "## 4.6 Techniques:\n",
    "\n",
    "4.6.1 Regularization (L1 for sparsity, L2 for weight decay)\n",
    "\n",
    "4.6.2 Contractive Penalty (to enforce robustness to small input perturbations)\n",
    "\n",
    "4.6.3 Reconstruction Loss (Mean Squared Error, Binary Cross-Entropy for binary data)\n",
    "\n",
    "4.6.4 KL Divergence (for VAEs to measure the difference between the learned distribution and the prior)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de2b09b",
   "metadata": {},
   "source": [
    "# All concept available we have to go through which is available in Neural Network\n",
    "## 1> Neurons\n",
    "\n",
    "## 2> Layers\n",
    "\n",
    "> 1.1> Input Layer\n",
    "\n",
    "> 1.2> Hidden Layers\n",
    "\n",
    "> 1.3> Output Layer\n",
    "\n",
    "## 3> Weights\n",
    "\n",
    "## 4> Bias\n",
    "\n",
    "## 5> Activation Function\n",
    "\n",
    "> 5.1> Sigmoid(it is used for binary class)\n",
    "\n",
    "> 5.2> Softmax (for multi-class classification)\n",
    "\n",
    "> 5.3> Tanh (Hyperbolic Tangent)\n",
    "\n",
    "> 5.4> ReLU (Rectified Linear Unit)\n",
    "\n",
    "> 5.5> LeakyReLU\n",
    "\n",
    "## 6> Loss Function (Cost Function)\n",
    "\n",
    "> 6.1> Binary Crossentropy\n",
    "\n",
    "> 6.2> Categorical Crossentropy\n",
    "\n",
    "> 6.3> Mean Squared Error (MSE)\n",
    "\n",
    "> 6.4> Mean Absolute Error (MAE)\n",
    "\n",
    "> 6.5> Huber Loss\n",
    "## 7> Optimizer\n",
    "\n",
    "> 7.1> Gradient Descent\n",
    "\n",
    "> 7.2> Stochastic Gradient Descent (SGD)\n",
    "\n",
    "> 7.3> Adam (Adaptive Moment Estimation)\n",
    "\n",
    "> 7.4> RMSprop (Root Mean Square Propagation)\n",
    "\n",
    "> 7.5> Adagrad (Adaptive Gradient Algorithm)\n",
    "## 8> Learning Rate\n",
    "\n",
    "## 9> Epoch : One complete pass through the entire training dataset.\n",
    "\n",
    "## 10> Batch Size\n",
    "\n",
    "## 11> Training Set\n",
    "\n",
    "## 12> Validation Set\n",
    "\n",
    "## 13> Test Set\n",
    "\n",
    "## 14> Forward Propagation\n",
    "\n",
    "## 15> Backward Propagation (Backpropagation)\n",
    "\n",
    "## 16> Gradient\n",
    "\n",
    "## 17> Dropout\n",
    "\n",
    "## 18> Regularization\n",
    "\n",
    "> 18.1> L1 Regularization (Lasso)\n",
    "\n",
    ">18.2> L2 Regularization (Ridge)\n",
    "\n",
    "## 19> Overfitting\n",
    "\n",
    "## 20> Underfitting\n",
    "\n",
    "## 21> Early Stopping\n",
    "\n",
    "## 22> Normalization/Standardization\n",
    "\n",
    "## 23> Epochs : The number of complete passes through the training dataset during the training process.\n",
    "\n",
    "## 24> Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8a0c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
