{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b6ee84",
   "metadata": {},
   "source": [
    "# What is Optimizer's ?\n",
    "## -> Optimizers are algorithms that adjust a neural network's parameters during training to minimize the model's error or loss function. They are a crucial part of deep learning because they help to:\n",
    "# 1) Improve performance :-\n",
    "## -> Optimizers help to improve the accuracy of predictions by finding the best set of parameters for the model.\n",
    "# 2) Speed up training :-\n",
    "## -> Optimizers help to speed up the training process by efficiently finding the optimal parameters. Without optimizers, training deep networks could be very slow and might get stuck in suboptimal solutions.\n",
    "# 3) Balance exploration and exploitation :-\n",
    "## -> Optimizers use various strategies to balance exploration and exploitation, helping to escape local minima and converge to optimal solutions.\n",
    "# 4) Choose the right optimizer :-\n",
    "## ->Selecting the right optimizer for the task at hand is important for achieving the best possible training results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b6ab9",
   "metadata": {},
   "source": [
    "# Important Deep Learning Terms :-\n",
    "## -> Before proceeding, there are a few terms that you should be familiar with.\n",
    "\n",
    ">Epoch – The number of times the algorithm runs on the whole training dataset.\n",
    "\n",
    ">Sample – A single row of a dataset.\n",
    "\n",
    ">Batch – It denotes the number of samples to be taken to for updating the model parameters.\n",
    "\n",
    ">Learning rate – It is a parameter that provides the model a scale of how much model weights should be updated.\n",
    "\n",
    ">Cost Function/Loss Function – A cost function is used to calculate the cost, which is the difference between the predicted value and the actual value.\n",
    "\n",
    ">Weights/ Bias – The learnable parameters in a model that controls the signal between two neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce6401",
   "metadata": {},
   "source": [
    "# TYPES OF OPTIMIZERS :\n",
    "\n",
    "## 1.) Gradient Descent\n",
    "## 2.) Stochastic Gradient Descent\n",
    "## 3.) Adagrad\n",
    "## 4.) Adadelta\n",
    "## 5.) RMSprop\n",
    "## 6.) Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33936c58",
   "metadata": {},
   "source": [
    "# 1) Gradient Descent :- \n",
    "## -> Gradient Descent can be considered the popular kid among the class of optimizers in deep learning. This optimization algorithm uses calculus to consistently modify the values and achieve the local minimum. Before moving ahead, you might question what a gradient is.\n",
    "\n",
    "## In simple terms, consider you are holding a ball resting at the top of a bowl. When you lose the ball, it goes along the steepest direction and eventually settles at the bottom of the bowl. A Gradient provides the ball in the steepest direction to reach the local minimum which is the bottom of the bowl.\n",
    "<a href=\"http://ibb.co/f8Epqx\"><img src=\"https://av-eks-blogoptimized.s3.amazonaws.com/31205gd6147.png\" alt=\"2\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3528a6",
   "metadata": {},
   "source": [
    "# -> There are 3 types of gradiant descent :- \n",
    "## 1.) Batch gradient descent\n",
    "## 2.) Stochastic gradient descent (SGD)\n",
    "## 3.) Mini-batch gradient descent                                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e02969",
   "metadata": {},
   "source": [
    "# 1.) Batch gradient descent :- \n",
    "## -> In Batch Gradient Descent, all the training data is taken into consideration to take a single step. We take the average of the gradients of all the training examples and then use that mean gradient to update our parameters. So that’s just one step of gradient descent in one epoch.\n",
    "\n",
    "## Batch Gradient Descent is great for convex or relatively smooth error manifolds. In this case, we move somewhat directly towards an optimum solution.\n",
    "\n",
    "<a href=\"http://ibb.co/f8Epqx\"><img src=\"https://miro.medium.com/v2/resize:fit:750/format:webp/1*44QbDJ9gJvw8tXtHNVLoCA.png\" alt=\"2\" border=\"0\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75458d05",
   "metadata": {},
   "source": [
    "## The graph of cost v/s epochs is also quite smooth because we are averaging over all the gradients of training data for a single step. The cost keeps on decreasing over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6528c91b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
